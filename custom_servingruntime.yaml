apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: custom-mlserver-1.x
  namespace: modelmesh-serving
  labels:
    name: modelmesh-serving-custom-mlserver-1.x-SR
spec:
  supportedModelFormats:
    - name: custom
      version: "1" # v1.0.x
      autoSelect: true

  # protocolVersions:
  #   - grpc-v2
  multiModel: true
  storageHelper:
    disabled: false
  replicas: 1
  grpcEndpoint: "port:8085"
  grpcDataEndpoint: "port:8001"

  containers:
    - name: mlserver
      image: lizzzcai/my-custom-mlserver:0.0.5
      env:
        - name: MLSERVER_MODELS_DIR
          value: "/models/"
        - name: MLSERVER_GRPC_PORT
          value: "8001"
        # default value for HTTP port is 8080 which conflicts with MMesh's
        # Litelinks port
        - name: MLSERVER_HTTP_PORT
          value: "8002"
        - name: MLSERVER_LOAD_MODELS_AT_STARTUP
          value: "false"
        # Set a dummy model name via environment so that MLServer doesn't
        # error on a RepositoryIndex call when no models exist
        - name: MLSERVER_MODEL_NAME
          value: dummy-model-fixme
        # Set server addr to localhost to ensure MLServer only listen inside the pod
        - name: MLSERVER_HOST
          value: "127.0.0.1"
        # Increase gRPC max message size to 16 MiB to support larger payloads
        - name: MLSERVER_GRPC_MAX_MESSAGE_LENGTH
          value: "16777216"
        - name: MLSERVER_DEBUG
          value: "true"
        - name: MLSERVER_MODEL_PARALLEL_WORKERS
          value: "0"

      resources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: "2"
          memory: 1Gi
  builtInAdapter:
    serverType: "mlserver"
    runtimeManagementPort: 8001
    memBufferBytes: 134217728
    modelLoadingTimeoutMillis: 90000
